# FILE: prepare_data_advanced.py
import re
import json
import os
import random
import string  # <--- ÄÃƒ THÃŠM DÃ’NG NÃ€Y (QUAN TRá»ŒNG)
import pandas as pd
from datasets import load_dataset, concatenate_datasets, Dataset
from unidecode import unidecode
from collections import defaultdict

# ==============================================================================
# Cáº¤U HÃŒNH
# ==============================================================================
RAW_DIR = "./scientific_data"
PROCESSED_DIR = "./data"
# TÃªn file Ä‘Ã£ khá»›p vá»›i hÃ¬nh áº£nh cá»§a báº¡n
DICT_PATH = os.path.join(RAW_DIR, "vi-nsw-dict.json")

os.makedirs(PROCESSED_DIR, exist_ok=True)

# 1. Báº¢NG MÃƒ Lá»–I GÃ• (TYPO)
TYPO_MAP = {
    'Ã¡': ['a1', 'as'], 'Ã ': ['a2', 'af'], 'áº£': ['a3', 'ar'], 'Ã£': ['a4', 'ax'], 'áº¡': ['a5', 'aj'],
    'Äƒ': ['a8', 'aw'], 'áº¯': ['a81', 'aws'], 'áº±': ['a82', 'awf'], 'áº·': ['a85', 'awj'],
    'Ã¢': ['a6', 'aa'], 'áº¥': ['a61', 'aas'], 'áº§': ['a62', 'aaf'], 'áº­': ['a65', 'aaj'],
    'Ä‘': ['d9', 'dd'], 'Ã©': ['e1', 'es'], 'Ã¨': ['e2', 'ef'], 'áº»': ['e3', 'er'], 'áº½': ['e4', 'ex'], 'áº¹': ['e5', 'ej'],
    'Ãª': ['e6', 'ee'], 'áº¿': ['e61', 'ees'], 'á»': ['e62', 'eef'], 'á»‡': ['e65', 'eej'],
    'Ã­': ['i1', 'is'], 'Ã¬': ['i2', 'if'], 'á»‰': ['i3', 'ir'], 'Ä©': ['i4', 'ix'], 'á»‹': ['i5', 'ij'],
    'Ã³': ['o1', 'os'], 'Ã²': ['o2', 'of'], 'á»': ['o3', 'or'], 'Ãµ': ['o4', 'ox'], 'á»': ['o5', 'oj'],
    'Ã´': ['o6', 'oo'], 'á»‘': ['o61', 'oos'], 'á»“': ['o62', 'oof'], 'á»™': ['o65', 'ooj'],
    'Æ¡': ['o7', 'ow'], 'á»›': ['o71', 'ows'], 'á»': ['o72', 'owf'], 'á»£': ['o75', 'owj'],
    'Ãº': ['u1', 'us'], 'Ã¹': ['u2', 'uf'], 'á»§': ['u3', 'ur'], 'Å©': ['u4', 'ux'], 'á»¥': ['u5', 'uj'],
    'Æ°': ['u7', 'uw'], 'á»©': ['u71', 'uws'], 'á»«': ['u72', 'uwf'], 'á»±': ['u75', 'uwj'],
    'Ã½': ['y1', 'ys'], 'á»³': ['y2', 'yf'], 'á»·': ['y3', 'yr'], 'á»¹': ['y4', 'yx'], 'á»µ': ['y5', 'yj']
}

# 2. LOAD & Äáº¢O NGÆ¯á»¢C Tá»ª ÄIá»‚N
print("âš™ï¸ ÄANG LOAD VÃ€ Äáº¢O NGÆ¯á»¢C Tá»ª ÄIá»‚N...")
rev_teencode_dict = defaultdict(list)

if os.path.exists(DICT_PATH):
    try:
        with open(DICT_PATH, "r", encoding="utf-8") as f:
            content = json.load(f)
            if isinstance(content, dict):
                for teencode, standard_list in content.items():
                    k = teencode.lower().strip()
                    vals = standard_list if isinstance(standard_list, list) else [standard_list]
                    for v in vals:
                        v_clean = v.lower().strip()
                        rev_teencode_dict[v_clean].append(k)
            else:
                print("âš ï¸ Tá»« Ä‘iá»ƒn khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng dict.")
    except Exception as e:
        print(f"âš ï¸ Lá»—i Ä‘á»c file tá»« Ä‘iá»ƒn: {e}")
else:
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file tá»« Ä‘iá»ƒn.")

print(f"âœ… ÄÃ£ load {len(rev_teencode_dict)} tá»« chuáº©n cÃ³ thá»ƒ map sang teencode.")

# HÃ m sinh lá»—i gÃµ
def simulate_typo(word):
    chars = list(word)
    new_chars = []
    has_changed = False
    for char in chars:
        if char in TYPO_MAP and random.random() < 0.3:
            typo_char = random.choice(TYPO_MAP[char])
            new_chars.append(typo_char)
            has_changed = True
        else:
            new_chars.append(char)
    return "".join(new_chars) if has_changed else word

# HÃ m Augmentation
def augment_text_advanced(text, ratio=0.5):
    if not isinstance(text, str): return ""
    
    words = text.split()
    new_words = []
    
    for word in words:
        # Cáº§n import string Ä‘á»ƒ dÃ¹ng string.punctuation á»Ÿ Ä‘Ã¢y
        clean_word = word.strip(string.punctuation).lower()
        rand = random.random()
        
        # 1. Teencode Reverse
        if clean_word in rev_teencode_dict and rand < 0.4:
            teencode_options = rev_teencode_dict[clean_word]
            chosen_teencode = random.choice(teencode_options)
            if word[0].isupper():
                chosen_teencode = chosen_teencode.capitalize()
            
            prefix = word[:len(word)-len(word.lstrip(string.punctuation))]
            suffix = word[len(word.rstrip(string.punctuation)):]
            new_words.append(prefix + chosen_teencode + suffix)
            
        # 2. Unidecode
        elif rand < 0.6:
            new_words.append(unidecode(word).lower())
            
        # 3. Typo
        elif rand < 0.7:
             new_words.append(simulate_typo(word))
             
        # 4. Giá»¯ nguyÃªn
        else:
            new_words.append(word)
            
    return " ".join(new_words)

# ==============================================================================
# Xá»¬ LÃ Dá»® LIá»†U
# ==============================================================================
all_datasets = []

# --- NHÃ“M 1: PARALLEL DATA ---
print("ðŸ”¹ Xá»­ lÃ½ Parallel Data (ViLexNorm, VSEC)...")

# 1.1 ViLexNorm
try:
    path_vilex = os.path.join(RAW_DIR, "vilexnorm.jsonl")
    if os.path.exists(path_vilex):
        ds = load_dataset("json", data_files=path_vilex, split="train")
        ds_clean = ds.map(lambda x: {"input": x["original"], "output": x["normalized"]}, 
                          remove_columns=ds.column_names)
        all_datasets.append(ds_clean)
        print(f"   - ÄÃ£ thÃªm ViLexNorm: {len(ds_clean)} máº«u.")
except Exception as e: print(f"âš ï¸ ViLexNorm Error: {e}")

# 1.2 VSEC
try:
    path_vsec = os.path.join(RAW_DIR, "vsec.jsonl")
    if os.path.exists(path_vsec):
        ds_vsec = load_dataset("json", data_files=path_vsec, split="train")
        def map_vsec(x):
            out_text = x["corrected_text"] if x["corrected_text"] else x["text"]
            return {"input": x["text"], "output": out_text}
        
        ds_vsec_clean = ds_vsec.map(map_vsec, remove_columns=ds_vsec.column_names)
        all_datasets.append(ds_vsec_clean)
        print(f"   - ÄÃ£ thÃªm VSEC: {len(ds_vsec_clean)} máº«u.")
except Exception as e: print(f"âš ï¸ VSEC Error: {e}")


# --- NHÃ“M 2: CONTEXT DATA ---
print("ðŸ”¹ Xá»­ lÃ½ Context Data (ViHSD, VSMEC)...")
context_files = ["vihsd.jsonl", "vsmec.jsonl"]
context_data = []

for fname in context_files:
    fpath = os.path.join(RAW_DIR, fname)
    if os.path.exists(fpath):
        print(f"   -> Äang Ä‘á»c {fname}...")
        with open(fpath, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    item = json.loads(line)
                    text = item.get("free_text") or item.get("sentence") or item.get("text")
                    if text and isinstance(text, str) and len(text.split()) > 3:
                        fake_input = augment_text_advanced(text, ratio=0.8)
                        context_data.append({"input": fake_input, "output": text})
                except Exception as e: 
                    # In lá»—i náº¿u cÃ³ Ä‘á»ƒ debug
                    # print(e) 
                    continue

if context_data:
    limit = min(len(context_data), 30000)
    ds_context = Dataset.from_pandas(pd.DataFrame(context_data)).shuffle(seed=42).select(range(limit))
    all_datasets.append(ds_context)
    print(f"   - ÄÃ£ táº¡o giáº£ láº­p tá»« Context Data: {len(ds_context)} máº«u.")
else:
    print("âš ï¸ Cáº£nh bÃ¡o: KhÃ´ng táº¡o Ä‘Æ°á»£c máº«u Context Data nÃ o (Kiá»ƒm tra láº¡i import string hoáº·c cáº¥u trÃºc file).")


# --- NHÃ“M 3: KNOWLEDGE DATA ---
print("ðŸ”¹ Xá»­ lÃ½ Knowledge Data (WikiAnn, WikiLingua)...")
knowledge_files = ["wikiann_ner.jsonl", "wikilingua.jsonl"] 
knowledge_data = []

def remove_punctuation(text):
    return re.sub(r'[^\w\s]', '', text)

def detokenize_naive(tokens):
    text = " ".join(tokens)
    text = re.sub(r'\s+([,.:;?!])', r'\1', text)
    return text

for fname in knowledge_files:
    fpath = os.path.join(RAW_DIR, fname)
    if os.path.exists(fpath):
        with open(fpath, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    item = json.loads(line)
                    text = ""
                    if "article" in item:
                         doc = item["article"].get("document", [])
                         if isinstance(doc, list): text = " ".join(doc)
                         elif isinstance(doc, str): text = doc
                    elif "tokens" in item:
                        text = detokenize_naive(item["tokens"])

                    if text and len(text) > 10:
                        text = text[:600]
                        no_tone_text = unidecode(text).lower()
                        clean_input = remove_punctuation(no_tone_text)
                        clean_input = " ".join(clean_input.split())
                        knowledge_data.append({"input": clean_input, "output": text})
                except: continue

if knowledge_data:
    ds_know = Dataset.from_pandas(pd.DataFrame(knowledge_data)).shuffle(seed=42).select(range(min(len(knowledge_data), 10000)))
    all_datasets.append(ds_know)
    print(f"   - ÄÃ£ xá»­ lÃ½ Knowledge Data: {len(ds_know)} máº«u.")

# 3. LÆ¯U FILE
if not all_datasets:
    print("âŒ Lá»–I: KhÃ´ng cÃ³ dá»¯ liá»‡u input!")
    exit()

print("ðŸ”¹ Äang gá»™p vÃ  trá»™n dá»¯ liá»‡u...")
full_dataset = concatenate_datasets(all_datasets).shuffle(seed=42)
split_ds = full_dataset.train_test_split(test_size=0.1)

print(f"âœ… Tá»”NG Cá»˜NG: {len(full_dataset)} máº«u.")
print(f"   - Train: {len(split_ds['train'])} -> lÆ°u táº¡i {PROCESSED_DIR}/train.jsonl")
print(f"   - Valid: {len(split_ds['test'])} -> lÆ°u táº¡i {PROCESSED_DIR}/valid.jsonl")

split_ds["train"].to_json(os.path.join(PROCESSED_DIR, "train.jsonl"), force_ascii=False)
split_ds["test"].to_json(os.path.join(PROCESSED_DIR, "valid.jsonl"), force_ascii=False)

print("ðŸŽ‰ DONE! Sáºµn sÃ ng train.")