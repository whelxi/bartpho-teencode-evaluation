import numpy as np
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
import evaluate
from peft import LoraConfig, get_peft_model, TaskType, PeftModel

# --- CÃC HÃ€M PHá»¤ TRá»¢ ---
def compute_metrics(eval_preds, tokenizer, metric):
    preds, labels = eval_preds
    if isinstance(preds, tuple): preds = preds[0]
    
    # === [FIX QUAN TRá»ŒNG] ===
    # Trainer chÃ¨n -100 vÃ o preds Ä‘á»ƒ padding, BARTpho decode bá»‹ lá»—i.
    # Cáº§n thay -100 vá» pad_token_id trÆ°á»›c khi decode.
    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)
    # ========================

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    
    # Xá»­ lÃ½ labels (báº¡n Ä‘Ã£ lÃ m Ä‘Ãºng pháº§n nÃ y, giá»¯ nguyÃªn)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    
    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}

def preprocess_function(examples, tokenizer):
    inputs = [str(x) for x in examples["input"]]
    targets = [str(x) for x in examples["output"]]
    
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
    
    # Tokenize labels
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    
    # Xá»­ lÃ½ padding cá»§a labels thÃ nh -100
    labels_ids = labels["input_ids"]
    labels_ids = [
        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels_ids
    ]
    
    model_inputs["labels"] = labels_ids
    return model_inputs

def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || "
        f"trainable%: {100 * trainable_params / all_param:.2f}%"
    )

# --- CHÆ¯Æ NG TRÃŒNH CHÃNH ---
if __name__ == "__main__":
    # Cáº¤U HÃŒNH
    MODEL_NAME = "vinai/bartpho-syllable"
    OUTPUT_DIR = "./bartpho-teencode-lora"
    DATA_DIR = "./data"

    print(f"ğŸš€ Báº¯t Ä‘áº§u Training vá»›i LoRA trÃªn RTX 3070 Ti...")
    
    # 1. Load Tokenizer & Model
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, dtype=torch.bfloat16)

    # --- [FIX 1] Táº¯t cache Ä‘á»ƒ trÃ¡nh xung Ä‘á»™t vá»›i Gradient Checkpointing ---
    model.config.use_cache = False 

    # --- [FIX 2] Chá»‰ báº­t input grad, khÃ´ng gá»i gradient_checkpointing_enable() thá»§ cÃ´ng á»Ÿ Ä‘Ã¢y
    # HÃ£y Ä‘á»ƒ TrainingArguments lÃ m viá»‡c Ä‘Ã³ Ä‘á»ƒ Ä‘á»“ng bá»™ config
    model.enable_input_require_grads()

    # --- TÃCH Há»¢P LORA ---
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_2_SEQ_LM, 
        inference_mode=False, 
        r=32,           
        lora_alpha=64,  
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "out_proj"] 
    )
    
    model = get_peft_model(model, peft_config)
    print("ğŸ“Š Thá»‘ng kÃª tham sá»‘ LoRA:")
    print_trainable_parameters(model)
    
    # 2. Load Data (Giá»¯ nguyÃªn)
    data_files = {"train": os.path.join(DATA_DIR, "train.jsonl"), 
                  "validation": os.path.join(DATA_DIR, "valid.jsonl")}
    dataset = load_dataset("json", data_files=data_files)

    # 3. Preprocess 
    tokenized_datasets = dataset.map(
        preprocess_function, 
        batched=True, 
        remove_columns=["input", "output"],
        fn_kwargs={"tokenizer": tokenizer} 
    )
    eval_subset = tokenized_datasets["validation"].shuffle(seed=42).select(range(200))

    # 4. Metric (Giá»¯ nguyÃªn)
    metric = evaluate.load("sacrebleu")
    def compute_metrics_wrapper(eval_preds):
        return compute_metrics(eval_preds, tokenizer, metric)

    # 5. Training Arguments (ÄÃƒ Sá»¬A)
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,  
        eval_strategy="steps",
        save_strategy="steps",
        
        predict_with_generate=True,  
        generation_max_length=128,   # Äá»™ dÃ i cÃ¢u tá»‘i Ä‘a khi dá»‹ch thá»­

        # --- [CHANGE 1] GIáº¢M BATCH SIZE XUá»NG ---
        # 3070 Ti 8GB khÃ¡ cháº­t chá»™i, giáº£m train xuá»‘ng 8 vÃ  eval xuá»‘ng 2
        per_device_train_batch_size=8,  # Giáº£m tá»« 16 -> 8
        per_device_eval_batch_size=2,   # Giáº£m tá»« 4 -> 2
        
        # --- [CHANGE 2] TÄ‚NG ACCUMULATION Äá»‚ BÃ™ Láº I BATCH SIZE ---
        # CÅ©: 16 * 2 = 32 máº«u/láº§n update. Má»›i: 8 * 4 = 32 máº«u/láº§n update.
        # Káº¿t quáº£ train tÆ°Æ¡ng Ä‘Æ°Æ¡ng nhÆ°ng tá»‘n Ã­t RAM hÆ¡n.
        gradient_accumulation_steps=8,  
        
        # --- [CHANGE 3] QUAN TRá»ŒNG CHO EVALUATION ---
        # Máº·c Ä‘á»‹nh Trainer sáº½ giá»¯ toÃ n bá»™ káº¿t quáº£ dá»± Ä‘oÃ¡n trÃªn GPU cho Ä‘áº¿n khi eval xong.
        # Set = 1 Ä‘á»ƒ nÃ³ Ä‘áº©y káº¿t quáº£ vá» CPU ngay láº­p tá»©c sau má»—i step, giáº£i phÃ³ng VRAM.
        eval_accumulation_steps=1,
        
        # === [FIX 3] Cáº¤U HÃŒNH QUAN TRá»ŒNG Äá»‚ Sá»¬A Lá»–I ===
        gradient_checkpointing=True, 
        gradient_checkpointing_kwargs={'use_reentrant': False}, # <--- DÃ’NG NÃ€Y Sáº¼ FIX Lá»–I "element 0"
        # ===============================================
        
        fp16=False,             
        bf16=True,              
        optim="adamw_torch",   
        dataloader_num_workers=0, # Windows fix (Giá»¯ nguyÃªn)
        
        group_by_length=True,
        learning_rate=3e-4, 
        num_train_epochs=5,
        logging_steps=50,
        save_steps=200,
        eval_steps=200,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="bleu",
        report_to="none"
    )

    trainer = Seq2SeqTrainer(
        model=model, 
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=eval_subset, 
        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),
        processing_class=tokenizer, 
        compute_metrics=compute_metrics_wrapper,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] 
    )

    # 6. Start Train
    print("ğŸ”¥ Äang tiáº¿n hÃ nh training...")
    trainer.train()

    # 7. Save Final
    print("ğŸ’¾ Äang lÆ°u Adapter model...")         
    trainer.save_model(OUTPUT_DIR)        
    tokenizer.save_pretrained(OUTPUT_DIR)   
    print(f"âœ… HoÃ n táº¥t! Model LoRA Ä‘Ã£ lÆ°u táº¡i: {OUTPUT_DIR}")