import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from peft import PeftModel

# Cáº¤U HÃŒNH
BASE_MODEL = "vinai/bartpho-syllable"
ADAPTER_PATH = "whelxi/bartpho-teencode"
NEW_REPO_NAME = "whelxi/bartpho-teencode-merged" # TÃªn model má»›i sáº½ táº¡o
HF_TOKEN = "" # Thay token Write cá»§a báº¡n vÃ o Ä‘Ã¢y

print("â³ Äang táº£i model (viá»‡c nÃ y tá»‘n RAM)...")

# 1. Load Base Model & Tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
base_model = AutoModelForSeq2SeqLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16, # DÃ¹ng float16 cho nháº¹
    device_map="auto"
)

# 2. Load Adapter
print("ğŸ”— Äang táº£i Adapter...")
model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)

# 3. Gá»™p (Merge) Adapter vÃ o Base Model
print("ğŸ”„ Äang gá»™p model (Merge & Unload)...")
model = model.merge_and_unload()

# 4. Upload lÃªn Hugging Face
print(f"â˜ï¸  Äang Ä‘áº©y model má»›i lÃªn: {NEW_REPO_NAME}...")
try:
    # Login thá»§ cÃ´ng náº¿u cáº§n, hoáº·c truyá»n token trá»±c tiáº¿p
    model.push_to_hub(NEW_REPO_NAME, token=HF_TOKEN, private=False) # Äá»ƒ private=False Ä‘á»ƒ dÃ¹ng API free
    tokenizer.push_to_hub(NEW_REPO_NAME, token=HF_TOKEN, private=False)
    print("âœ… THÃ€NH CÃ”NG! HÃ£y dÃ¹ng tÃªn model má»›i nÃ y trong file test_api.py")
except Exception as e:
    print(f"âŒ Lá»—i Upload: {e}")