1.vi-nsw-dict.json

2.vihsd.jsonl

3.vilexnorm.jsonl

4.vsec.jsonl

5.vsmec.jsonl

6.wikiann_ner.jsonl

7.wikilingua.jsonl




# old train.py
import numpy as np

import os

import torch

from datasets import load_dataset

from transformers import (

    AutoTokenizer, AutoModelForSeq2SeqLM,

    Trainer, TrainingArguments, DataCollatorForSeq2Seq,

    EarlyStoppingCallback

)

import evaluate



# --- CÃC HÃ€M PHá»¤ TRá»¢ ---



def compute_metrics(eval_preds, tokenizer, metric):

    preds, labels = eval_preds

    if isinstance(preds, tuple): preds = preds[0]

   

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

   

    decoded_preds = [pred.strip() for pred in decoded_preds]

    decoded_labels = [[label.strip()] for label in decoded_labels]

   

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)

    return {"bleu": result["score"]}



def preprocess_function(examples, tokenizer):

    inputs = [str(x) for x in examples["input"]]

    targets = [str(x) for x in examples["output"]]

   

    # Giáº£m max_length xuá»‘ng 100 náº¿u dá»¯ liá»‡u teencode ngáº¯n Ä‘á»ƒ tiáº¿t kiá»‡m VRAM (TÃ¹y chá»n)

    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")

    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")

   

    model_inputs["labels"] = labels["input_ids"]

    return model_inputs



# --- CHÆ¯Æ NG TRÃŒNH CHÃNH ---

if __name__ == "__main__":

    # Cáº¤U HÃŒNH

    MODEL_NAME = "vinai/bartpho-syllable"

    OUTPUT_DIR = "./bartpho-teencode-v2"

    DATA_DIR = "./data"



    print(f"ğŸš€ Báº¯t Ä‘áº§u Training tá»‘i Æ°u hÃ³a cho RTX 3070 Ti...")

    print(f"CUDA available: {torch.cuda.is_available()}")

    if torch.cuda.is_available():

        print(f"GPU: {torch.cuda.get_device_name(0)}")



    # 1. Load Tokenizer & Model

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)



    # 2. Load Data

    data_files = {"train": os.path.join(DATA_DIR, "train.jsonl"),

                  "validation": os.path.join(DATA_DIR, "valid.jsonl")}

    dataset = load_dataset("json", data_files=data_files)



    # 3. Preprocess

    tokenized_datasets = dataset.map(

        preprocess_function,

        batched=True,

        remove_columns=["input", "output"],

        fn_kwargs={"tokenizer": tokenizer}

    )



    # 4. Metric

    metric = evaluate.load("sacrebleu")

    def compute_metrics_wrapper(eval_preds):

        return compute_metrics(eval_preds, tokenizer, metric)



    # 5. Training Arguments (Tá»I Æ¯U HÃ“A)

    training_args = TrainingArguments(

        output_dir=OUTPUT_DIR,

        eval_strategy="steps",

        save_strategy="steps",

       

        # --- Cáº¤U HÃŒNH Tá»° Äá»˜NG & Tá»I Æ¯U VRAM ---

        auto_find_batch_size=True,  # Tá»± Ä‘á»™ng dÃ² tÃ¬m batch size lá»›n nháº¥t cÃ³ thá»ƒ (Báº¯t Ä‘áº§u tá»« power of 2)

        # per_device_train_batch_size=8, # DÃ²ng nÃ y sáº½ bá»‹ auto_find_batch_size ghi Ä‘Ã¨

        per_device_eval_batch_size=8,    # Eval tá»‘n Ã­t VRAM hÆ¡n nÃªn cÃ³ thá»ƒ set cá»©ng

       

        gradient_accumulation_steps=4,   # Giáº£m xuá»‘ng vÃ¬ batch size tháº­t sáº½ tÄƒng lÃªn

        gradient_checkpointing=True,     # Tiáº¿t kiá»‡m VRAM Ä‘á»•i láº¥y chÃºt tá»‘c Ä‘á»™ tÃ­nh toÃ¡n (NÃªn báº­t vá»›i 8GB VRAM)

       

        # --- Tá»I Æ¯U Tá»C Äá»˜ CHO RTX 30xx ---

        tf32=True,             # TÄƒng tá»‘c trÃªn card Ampere (3070 Ti)

        fp16=True,             # Mixed precision

       

        # --- Cáº¤U HÃŒNH DATA LOADING (WINDOWS) ---

        dataloader_num_workers=0,    # GIá»® NGUYÃŠN 0 TRÃŠN WINDOWS Äá»‚ TRÃNH CRASH

        dataloader_pin_memory=True,  # TÄƒng tá»‘c copy dá»¯ liá»‡u tá»« RAM -> VRAM

        group_by_length=True,        # Gom nhÃ³m dá»¯ liá»‡u cÃ¹ng Ä‘á»™ dÃ i -> Training nhanh hÆ¡n

        # ----------------------------------------



        learning_rate=2e-5,

        num_train_epochs=10, # Giáº£m epoch xuá»‘ng 10 xem káº¿t quáº£ trÆ°á»›c, 18 hÆ¡i nhiá»u

       

        logging_steps=50,    # Log thÆ°á»ng xuyÃªn hÆ¡n Ä‘á»ƒ tháº¥y tá»‘c Ä‘á»™

        save_steps=200,

        eval_steps=200,

        save_total_limit=2,

        load_best_model_at_end=True,

        metric_for_best_model="bleu",

        report_to="none"

    )



    trainer = Trainer(

        model=model,

        args=training_args,

        train_dataset=tokenized_datasets["train"],

        eval_dataset=tokenized_datasets["validation"],

        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),

        processing_class=tokenizer,

        compute_metrics=compute_metrics_wrapper,

        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]

    )



    # 6. Start Train

    print("ğŸ”¥ Äang tiáº¿n hÃ nh training (Giai Ä‘oáº¡n Ä‘áº§u sáº½ hÆ¡i lÃ¢u Ä‘á»ƒ dÃ² Batch Size)...")

    trainer.train()



    # 7. Save Final

    print("ğŸ’¾ Äang lÆ°u model...")        

    trainer.save_model(OUTPUT_DIR)        

    tokenizer.save_pretrained(OUTPUT_DIR)  

    print(f"âœ… HoÃ n táº¥t! Model Ä‘Ã£ lÆ°u táº¡i: {OUTPUT_DIR}")

# New train.py
import numpy as np
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    Trainer, TrainingArguments, DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
import evaluate
from peft import LoraConfig, get_peft_model, TaskType, PeftModel

# --- CÃC HÃ€M PHá»¤ TRá»¢ ---
def compute_metrics(eval_preds, tokenizer, metric):
    preds, labels = eval_preds
    if isinstance(preds, tuple): preds = preds[0]
    
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    
    # Thay tháº¿ -100 vá» láº¡i pad_token_id Ä‘á»ƒ decode Ä‘Æ°á»£c
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    
    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}

def preprocess_function(examples, tokenizer):
    inputs = [str(x) for x in examples["input"]]
    targets = [str(x) for x in examples["output"]]
    
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
    
    # Tokenize labels
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    
    # Xá»­ lÃ½ padding cá»§a labels thÃ nh -100
    labels_ids = labels["input_ids"]
    labels_ids = [
        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels_ids
    ]
    
    model_inputs["labels"] = labels_ids
    return model_inputs

def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || "
        f"trainable%: {100 * trainable_params / all_param:.2f}%"
    )

# --- CHÆ¯Æ NG TRÃŒNH CHÃNH ---
if __name__ == "__main__":
    # Cáº¤U HÃŒNH
    MODEL_NAME = "vinai/bartpho-syllable"
    OUTPUT_DIR = "./bartpho-teencode-lora"
    DATA_DIR = "./data"

    print(f"ğŸš€ Báº¯t Ä‘áº§u Training vá»›i LoRA trÃªn RTX 3070 Ti...")
    
    # 1. Load Tokenizer & Model
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, dtype=torch.bfloat16)

    # --- [FIX 1] Táº¯t cache Ä‘á»ƒ trÃ¡nh xung Ä‘á»™t vá»›i Gradient Checkpointing ---
    model.config.use_cache = False 

    # --- [FIX 2] Chá»‰ báº­t input grad, khÃ´ng gá»i gradient_checkpointing_enable() thá»§ cÃ´ng á»Ÿ Ä‘Ã¢y
    # HÃ£y Ä‘á»ƒ TrainingArguments lÃ m viá»‡c Ä‘Ã³ Ä‘á»ƒ Ä‘á»“ng bá»™ config
    model.enable_input_require_grads()

    # --- TÃCH Há»¢P LORA ---
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_2_SEQ_LM, 
        inference_mode=False, 
        r=32,           
        lora_alpha=64,  
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "out_proj"] 
    )
    
    model = get_peft_model(model, peft_config)
    print("ğŸ“Š Thá»‘ng kÃª tham sá»‘ LoRA:")
    print_trainable_parameters(model)
    
    # 2. Load Data (Giá»¯ nguyÃªn)
    data_files = {"train": os.path.join(DATA_DIR, "train.jsonl"), 
                  "validation": os.path.join(DATA_DIR, "valid.jsonl")}
    dataset = load_dataset("json", data_files=data_files)

    # 3. Preprocess (Giá»¯ nguyÃªn)
    tokenized_datasets = dataset.map(
        preprocess_function, 
        batched=True, 
        remove_columns=["input", "output"],
        fn_kwargs={"tokenizer": tokenizer} 
    )

    # 4. Metric (Giá»¯ nguyÃªn)
    metric = evaluate.load("sacrebleu")
    def compute_metrics_wrapper(eval_preds):
        return compute_metrics(eval_preds, tokenizer, metric)

    # 5. Training Arguments (ÄÃƒ Sá»¬A)
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        eval_strategy="steps",
        save_strategy="steps",
        
        per_device_train_batch_size=16, 
        gradient_accumulation_steps=2, 
        per_device_eval_batch_size=4,   
        
        # === [FIX 3] Cáº¤U HÃŒNH QUAN TRá»ŒNG Äá»‚ Sá»¬A Lá»–I ===
        gradient_checkpointing=True, 
        gradient_checkpointing_kwargs={'use_reentrant': False}, # <--- DÃ’NG NÃ€Y Sáº¼ FIX Lá»–I "element 0"
        # ===============================================
        
        fp16=False,             
        bf16=True,              
        optim="adamw_torch",   
        dataloader_num_workers=0, # Windows fix (Giá»¯ nguyÃªn)
        
        group_by_length=True,
        learning_rate=3e-4, 
        num_train_epochs=5,
        logging_steps=50,
        save_steps=200,
        eval_steps=200,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="bleu",
        report_to="none"
    )

    trainer = Trainer(
        model=model, 
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"], 
        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),
        processing_class=tokenizer, 
        compute_metrics=compute_metrics_wrapper,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] 
    )

    # 6. Start Train
    print("ğŸ”¥ Äang tiáº¿n hÃ nh training...")
    trainer.train()

    # 7. Save Final
    print("ğŸ’¾ Äang lÆ°u Adapter model...")         
    trainer.save_model(OUTPUT_DIR)        
    tokenizer.save_pretrained(OUTPUT_DIR)   
    print(f"âœ… HoÃ n táº¥t! Model LoRA Ä‘Ã£ lÆ°u táº¡i: {OUTPUT_DIR}")